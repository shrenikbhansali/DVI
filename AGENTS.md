# AGENTS.md
<!--
  Read me first!  This file is the onâ€‘boarding document for any human
  *or* autonomous agent that touches the repo.
-->

---

## 0â€¯Â·â€¯What is Kangaroo & What is Speculativeâ€¯Decoding?Â *(60â€¯s primer)*

1. **Speculativeâ€¯Decoding (SD)**  
   An autoregressive acceleration trick:  
   *Draft* network proposes several tokens in one shot â†’ *Verifier* network
   checks them in parallel.  
   Accepted tokens are committed; rejected ones are replaced.  
   Result: **1.5â€“2â€¯Ã—** throughput without changing the final distribution.

2. **Kangaroo (Liâ€¯etâ€¯al., 2024)**  
   A *selfâ€‘speculative* variant: a **single Llama model** is split at layerâ€¯*k*.  
   LayersÂ 0â€‘k play the Draft role; layersÂ kâ€‘L play the Verifier role.  
   No auxiliary model, no extra memory. Â *(We start from this codebase.)*

---

## 1â€¯Â·â€¯Repo Origin & Immutable Core

This repository **forks the originalâ€¯â†’â€¯[`Equationliu/Kangaroo`](https://github.com/Equationliu/Kangaroo)**.  
We are layering **Draftâ€¯â†’â€¯Verifyâ€¯â†’â€¯Improve RL (DVIâ€‘RL)** on top.

> ğŸ”’ **Files that must remain functionally intact (only *surgical* edits allowed):**
>
> | Path | Why it must be stable |
> |------|-----------------------|
> | `kangaroo/earlyexit.py` | Houses the microâ€‘step speculative decoder; even small logic drift can break accept/reject signals. |
> | `kangaroo/adapter.py`   | Reâ€‘implements a slim 1â€‘layer decoder and **patches several HF internals** (RMSNorm, rotary cache, causal masks). Many helper methods live here; other modules must call these not HF defaults. |
> | `evaluation/inference_kangaroo.py` | Benchmark reference; changes here mask regression in speed/quality. |

Any PR that rewrites the algorithms inside these files **must** include:

1. A benchmark replicating Kangarooâ€™s original speedâ€‘up on `--max_new_tokens 64`.
2. Unit tests (`pytest -q`) proving unchanged acceptâ€‘rate on a fixed seed batch.

---

## 2â€¯Â·â€¯Always CheckÂ `adapter.py` First â—
We are **NOT** using the latest version of huggingface / transformers. We are using transformers=4.33.3. The version that you have installed *is* is the latest version. Therefore, you should check against the web or check against `kangaroo/adapter.py`. 

`kangaroo/adapter.py` is **not** a thin wrapper:

* It is the transformers=4.33.3 implementations of **LlamaAttention**, **LlamaRotaryEmbedding**, causal masking, etc.
* Caps KVâ€‘cache toÂ 64 tokens for memory.
* Provides `_set_cos_sin_cache()`, `forward_early_stop()`, `_prepare_decoder_attention_mask()`â€”utilities that the rest of the codebase *will* call.

â¡ï¸ **Before you import or extend any HF module, search `adapter.py` to avoid
silent incompatibilities.**

---

## 3â€¯Â·â€¯Bigâ€‘Picture Vision

This repo turns speculative decoding into a **trainingâ€‘time signal**, not just an inference trick.

### ğŸ” Draft â†’ Verify â†’ Improve (DVI-RL)

| Role | Description |
|------|-------------|
| **Draft** | Shallow layers predict next token with `lora_S` adapters |
| **Verify** | Deep layers accept/reject the draft with `lora_D` adapters |
| **Improve** | Draft gets REINFORCE signal on accept=1 / reject=0 |

This creates a **self-contained RL loop**:  
no external reward, no labels, no human supervision.

### ğŸ¤– Practical impact

Train from **streaming conversation** logs (e.g. ShareGPT):  
no replay buffer, no human annotators, and constant compute overhead.

---

## 4â€¯Â·â€¯Codebase Map

| Folder | Description |
|--------|-------------|
| `kangaroo/` | Core speculative decoding utilities: modified model, adapter patches, early exit logic |
| `evaluation/` | Inference benchmarking, logging, acceptance tracing |
| `training/` | SGP, DVI-RL trainers, slow-update routines |
| `lora/` | LoRA injection & separation of shallow/deep adapters |
| `buffer/` | Replay buffer used by fast and slow optimizers |
| `scripts/` | CLI entrypoints for train/eval/trace |
| `configs/` | YAML files for experiment reproducibility |

---

## 5â€¯Â·â€¯Interfaces & Contracts

### `EarlyExitLlamaForCausalLM` (in `earlyexit.py`)

- `forward_draft_or_large_model(...)`: core forward pass, either shallow or deep
- `spec_decode_step(...)`: single step of speculative decoding with accept bit

### `adapter.py`

- Overloads HF modules: do not bypass these!
- Provides `LlamaForCausalLMWithAdapters` and `inject_adapter_hooks(...)`

### Replay Buffer (`buffer.py`)

- Fixedâ€‘capacity FIFO buffer that stores accepted token transitions
- Used for fast policy gradient and slow verifier fine-tuning

---

## 6â€¯Â·â€¯Development Guidelines

- âœ… Follow modularity: isolate logic in `trainer_dvi.py`, keep inference clean
- âœ… Add metrics to `evaluation/trace_logger.py` rather than printing manually
- âœ… Add new adapters via `inject_dual_lora(...)` (see `sgp_lora.py`)
- âœ… Unit test slowâ€‘update schedules separately
- â— Respect **immutable files** (`earlyexit.py`, `adapter.py`, `inference_kangaroo.py`)
- â— Always check if a helper exists in `adapter.py` before modifying or reâ€‘writing core logic (especially attention, masks, or embeddings)

---
